{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from combat.pycombat import pycombat\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, RocCurveDisplay, matthews_corrcoef\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from time import time\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv_files(path, starts_with, ends_with = \".csv\"):\n",
    "    \"\"\"Imports csv files based on path and starting characters.\n",
    "\n",
    "    :param path: Path of where to find csv files.\n",
    "    :param starts_with: Starting characters of dataframes to find.\n",
    "    :param ends_with: Type of file to load.\n",
    "    \n",
    "    :return: List of loaded csv files.\"\"\"    \n",
    "    file_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.startswith(starts_with) and filename.endswith(ends_with):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            file_list.append(pd.read_csv(file_path))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_shape(dfs : list):\n",
    "    \"\"\"Checks if all dataframes in a list have the same shape.\n",
    "\n",
    "    :param dfs: List of dataframes.\n",
    "    \n",
    "    :return: Boolean value if all dataframes have the same shape.\"\"\"    \n",
    "    return all(df.shape == dfs[0].shape for df in dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_from_m_datasets(dfs : list):\n",
    "    \"\"\"Pools imputed dataset into a single dataframe.\n",
    "\n",
    "    :param dfs: List of dataframes.\n",
    "    \n",
    "    :return: Singular, pooled dataframe.\"\"\"    \n",
    "    if not same_shape:\n",
    "        print(\"Dataframes must have the same shape\")\n",
    "    joined = pd.concat(dfs).reset_index() \n",
    "    return joined.groupby('index').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary(df, y, keepers):\n",
    "    \"\"\"Removes classes from df and y that are not present in var keepers.\n",
    "\n",
    "    :param df: dataframe (X in classification task)\n",
    "    :param y: class labels\n",
    "    :param keepers: labels that we want to keep\n",
    "    :return: X and y without labels not present in keepers\n",
    "    \"\"\"    \n",
    "    mask = y.isin(keepers)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combat(df, TMT_set_indices):\n",
    "    \"\"\"Run ComBat to reduce batch effect on a dataframe.\n",
    "\n",
    "    :param df: Dataframe that ComBat is runned on.\n",
    "    :param TMT_set_indices: Labels of which TMT batch each row belongs to.\n",
    "    \n",
    "    :return: DF with ComBat applied to it.\"\"\"    \n",
    "    return pycombat(df.T, TMT_set_indices).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(df, n_samples, std = 0.1):\n",
    "    \"\"\"Helper function for simple perturbation function. Adds gaussian noise to randomly sampled rows of the df passed as an argument.\n",
    "\n",
    "    :param df: df filtered on class.\n",
    "    :param n_samples: Number of samples to perform random sampling on.\n",
    "    :param std: Standard deviation. Defaults to 0.1.\n",
    "    \n",
    "    :return: df with added perturbed samples.\"\"\"      \n",
    "    sampled_df = df.sample(n = n_samples, replace = True)\n",
    "    gaussian_noise = np.random.normal(0, std, size=sampled_df.shape)\n",
    "    return sampled_df + gaussian_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_perturbation(X, y, n_samples_per_class = None, std = 0.1):\n",
    "    \"\"\"Performs a simple perturbation by sampling random rows of classes and adds gaussian noise to them\n",
    "\n",
    "    :param X: Dataframe with samples to get perturbed.\n",
    "    :param y: Class labels.\n",
    "    :param n_samples_per_class: Decides how many samples per class that are going to be sampled, defaults to None. \n",
    "    If none -> all classes will get equal weight according to size of current largest class.\n",
    "    :param std: How much the noise can deviate from the mean (Standard dev.), defaults to 0.1\n",
    "    \n",
    "    :return: A df with the perturbed samples. A list stating which row belongs to which class.\"\"\"    \n",
    "    classes = y.value_counts()\n",
    "    if n_samples_per_class is None:\n",
    "        largest_class = classes.argmax()\n",
    "        largest_n_samples = classes.pop(largest_class)\n",
    "    else:\n",
    "        largest_n_samples = n_samples_per_class\n",
    "    df_list = []\n",
    "    y_new_classes = list(y)\n",
    "    for idx, n_samples in classes.items():\n",
    "        perturbed = add_noise(X[y == idx], largest_n_samples - n_samples, std=std)\n",
    "        df_list.append(perturbed)\n",
    "        y_new_classes = y_new_classes + [idx] * len(perturbed)\n",
    "    return pd.concat([X] + df_list, axis=0), y_new_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(X, y, models, params, cv = 5, n_jobs = -1):\n",
    "    \"\"\"Runs GridSearchCV in order to find best parameters on one or more models.\n",
    "\n",
    "    :param X: The training data for the models.\n",
    "    :param y: The correct classification of the training data.\n",
    "    :param models: A list of models to be trained.\n",
    "    :param params: A dictionary with the parameters for each model to be GridSearched.\n",
    "    :param cv: The K-fold cross validation value.\n",
    "    :param n_jobs: Number of cores to use for GridSearchCV.\n",
    "    \n",
    "    :return: DF with ComBat applied to it.\"\"\"   \n",
    "    best_params = {}\n",
    "    for model, param in zip(models, params):\n",
    "        start_time = time()\n",
    "        print(f\"\\033[97m{model.name} started.\")\n",
    "        clf = GridSearchCV(model, param_grid=param, cv=cv, n_jobs=n_jobs, scoring=\"f1\")\n",
    "        clf.fit(X, y)\n",
    "        best_params[model.name] = clf.best_params_\n",
    "        print(f\"{model.name} is done in {time() - start_time} seconds.\\n\" )\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params_bayes(X, y, models, params, n_iter = 50, cv = 3, n_jobs = -1):\n",
    "    \"\"\"Runs GridSearchCV in order to find best parameters on one or more models.\n",
    "\n",
    "    :param X: The training data for the models.\n",
    "    :param y: The correct classification of the training data.\n",
    "    :param models: A list of models to be trained.\n",
    "    :param params: A dictionary with the parameters for each model to be GridSearched.\n",
    "    :param cv: The K-fold cross validation value.\n",
    "    :param n_jobs: Number of cores to use for GridSearchCV.\n",
    "\n",
    "    :return: DF with ComBat applied to it\"\"\"   \n",
    "    best_params = {}\n",
    "    for model, param in zip(models, params):\n",
    "        start_time = time()\n",
    "        print(f\"\\033[97m{model.name} started.\")\n",
    "        clf = BayesSearchCV(model, search_spaces=param, n_iter=n_iter, cv=cv, n_jobs=n_jobs, scoring=\"f1\")\n",
    "        clf.fit(X, y)\n",
    "        best_params[model.name] = clf.best_params_\n",
    "        print(f\"{model.name} is done in {time() - start_time} seconds.\\n\" )\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_feature_importance_XGB_RF(models, df):\n",
    "    \"\"\"Extracts feature importance from the same models over multiple iterations, averaging them over each iteration.\n",
    "\n",
    "    :param models: Models to extract feature importance from.\n",
    "    :param df: Original dataframe in order to map feature importances to column names.\n",
    "    \n",
    "    :return: Dictionary with column name as keys and average feature importance as values.\"\"\"  \n",
    "    avg_feature_importance = {}\n",
    "    for model in models:\n",
    "        importances = model.feature_importances_\n",
    "        for index, importance in enumerate(importances):\n",
    "            name = df.columns[index]\n",
    "            if name in avg_feature_importance:\n",
    "                avg_feature_importance[name] += importance\n",
    "            else:\n",
    "                avg_feature_importance[name] = importance\n",
    "    for feature, total in avg_feature_importance.items():\n",
    "        mean = total / len(models)\n",
    "        avg_feature_importance[feature] = mean\n",
    "    \n",
    "    return avg_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_feature_importance_LR(models, df):\n",
    "    \"\"\"Extracts feature importance from the same models over multiple iterations, averaging them over each iteration.\n",
    "\n",
    "    :param models: Models to extract feature importance from.\n",
    "    :param df: Original dataframe in order to map feature importances to column names.\n",
    "    \n",
    "    :return: Dictionary with column name as keys and average feature importance as values.\"\"\"  \n",
    "    all_coefficients = []\n",
    "    feature_names = df.columns.tolist()\n",
    "    max_length = max(len(model.coef_[0]) for model in models)\n",
    "    \n",
    "    for model in models:\n",
    "        coefficients = model.coef_[0]\n",
    "        padded_coefficients = np.pad(coefficients, (0, max_length - len(coefficients)), mode='constant', constant_values=np.nan)\n",
    "        all_coefficients.append(padded_coefficients)\n",
    "\n",
    "    avg_coefficients = np.nanmean(all_coefficients, axis=0) \n",
    "    normalized_coefficients = avg_coefficients / np.nansum(avg_coefficients)\n",
    "    feature_importance = dict(zip(feature_names, normalized_coefficients))\n",
    "    \n",
    "    return feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_feature_importance(d, model_name, df):\n",
    "    \"\"\"Wrapper function in order to extract average feature importance across multiple iterations of multiple models.\n",
    "\n",
    "    :param d: Dictionary with models.\n",
    "    :param model_name: Name of the model.\n",
    "    :param df: Original dataframe in order to map feature importances to column names.\n",
    "    \n",
    "    :return: Dictionary with column name as keys and average feature importance as values.\"\"\"  \n",
    "    models = [model for key, model in d.items() if key.startswith(f'{model_name}')]\n",
    "    if model_name == \"LR\":\n",
    "        return get_average_feature_importance_LR(models, df)\n",
    "    return get_average_feature_importance_XGB_RF(models, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(dictionaries):\n",
    "    \"\"\"Gets the average feature importance from multiple dictionaries\n",
    "    \n",
    "    :param dictionaries: List of dictionaries to be averaged.\n",
    "\n",
    "    :return: Dictionary with averaged values.\"\"\"\n",
    "    sum_values = {}\n",
    "    count_values = {}\n",
    "    \n",
    "    for dictionary in dictionaries:\n",
    "        for key, value in dictionary.items():\n",
    "            if key not in sum_values:\n",
    "                sum_values[key] = 0\n",
    "                count_values[key] = 0\n",
    "            sum_values[key] += value\n",
    "            count_values[key] += 1\n",
    "    \n",
    "    averages = {}\n",
    "    for key, total_sum in sum_values.items():\n",
    "        averages[key] = total_sum / count_values[key]\n",
    "    \n",
    "    return averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_voting(models):\n",
    "    \"\"\"Hard voting based on predictions from multiple models.\n",
    "\n",
    "    :param models: List of trained models.\n",
    "    \n",
    "    :return: Hard voting results.\"\"\"  \n",
    "    predictions = [model.prediction for model in models]\n",
    "    predictions = [[row[i] for row in predictions] for i in range(len(predictions[0]))]\n",
    "    results = []\n",
    "    for index, p in enumerate(predictions):\n",
    "        most_common = Counter(p).most_common()\n",
    "        if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:\n",
    "            probabilities = [model.probability[index] for model in models]\n",
    "            probabilities = [[row[i] for row in probabilities] for i in range(len(probabilities[0]))]\n",
    "            probabilities = [sum(sublist) for sublist in probabilities]\n",
    "            if len(most_common) > 2 and most_common[0][1] == most_common[2][1]:\n",
    "                results.append(np.argmax(probabilities))\n",
    "            else:\n",
    "                probabilities[most_common[2][0]] = 0\n",
    "                results.append(np.argmax(probabilities))\n",
    "        else:\n",
    "            results.append(most_common[0][0])\n",
    "\n",
    "    return results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_voting(models):\n",
    "    \"\"\"Soft voting based on predictions from multiple models.\n",
    "\n",
    "    :param models: List of trained models.\n",
    "    \n",
    "    :return: Soft voting results.\"\"\"  \n",
    "    probabilities = [model.probability for model in models]\n",
    "    summed_probabilities = np.sum(probabilities, axis=0)\n",
    "\n",
    "    results = np.argmax(summed_probabilities, axis=1)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging(df_lumbar, df_ventricular, y_lumbar, y_ventricular, feature):\n",
    "    \"\"\"Plots boxplot staging of feature based on analyte and pathology.\n",
    "\n",
    "    :param df_lumbar: The lumbar part of the dataset.\n",
    "    :param df_ventricular: The ventricular part of the dataset.\n",
    "    :param y_lumbar: The lumbar part of the pathology class.\n",
    "    :param y_ventricular: The ventricular part of the pathology class.\n",
    "    :param feature: The protein or peptide that should be visualized.\"\"\"\n",
    "    \n",
    "    cn_pathology = \"A\\u03B2\\u207BT\\u207B\"  # cn_pathology\n",
    "    mci_pathology = \"A\\u03B2\\u207AT\\u207B\"  # mci_pathology\n",
    "    ad_pathology = \"A\\u03B2\\u207AT\\u207A\"   # ad_pathology\n",
    "\n",
    "    df_lumbar[\"CSF_type\"] = \"Lumbar\"\n",
    "    df_ventricular[\"CSF_type\"] = \"Ventricular\"\n",
    "\n",
    "    df_merged = pd.concat([df_ventricular, df_lumbar], ignore_index=True)\n",
    "    y_merged = pd.concat([y_lumbar, y_ventricular], ignore_index=True)\n",
    "    y_merged = y_merged.replace(0, cn_pathology)\n",
    "    if len(y_merged.value_counts()) > 2:\n",
    "        y_merged = y_merged.replace(1, mci_pathology)\n",
    "    y_merged = y_merged.replace(len(y_merged.value_counts())-1, ad_pathology)\n",
    "    \n",
    "    if len(y_merged.value_counts()) > 2:\n",
    "        ax = sns.boxplot(data=df_merged, x=\"CSF_type\", y=feature, hue=y_merged, hue_order=[cn_pathology, mci_pathology, ad_pathology]);\n",
    "    else: \n",
    "        ax = sns.boxplot(data=df_merged, x=\"CSF_type\", y=feature, hue=y_merged, hue_order=[cn_pathology, ad_pathology]);\n",
    "\n",
    "    ax.set_xlabel(\"CSF Sample Type\", fontsize = 15)\n",
    "    ax.set_ylabel(\"Abundance\", fontsize = 15)\n",
    "    ax.legend(loc=\"upper right\", title=\"Tissue group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentiles(percentiles, labels, title=\"Accuracy scores\", ax=None):\n",
    "    \"\"\"Plot percentiles against labels.\n",
    "\n",
    "    :param percentiles: List of percentiles.\n",
    "    :param labels: List of strings (labels).\n",
    "    :param title: Title of the plot.\n",
    "    :param ax: Axes object to plot on.\"\"\"\n",
    "    x = range(len(labels))\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.bar(x, percentiles, color='skyblue')\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(dict0, dict1, dict2, dict3, models=[\"Average\",\"Model1\", \"Model2\", \"Model3\"], main_model = None, colours = [\"tab:cyan\", \"tab:blue\", \"tab:orange\", \"tab:olive\"]):\n",
    "    \"\"\"Plots feature importance from three dictionaries.\n",
    "\n",
    "    :param dict: \n",
    "    :param labels: List of strings (labels).\"\"\"\n",
    "    sorted_dict0 = dict(sorted(dict0.items(), key=lambda item: item[1], reverse=True))\n",
    "    top_features_dict0 = list(sorted_dict0.items())[:5]\n",
    "    top_features_keys = [key for key, _ in top_features_dict0]\n",
    "\n",
    "    feature_importance_dict0 = [val for _, val in top_features_dict0]\n",
    "    feature_importance_dict1 = [dict1[key] for key in top_features_keys]\n",
    "    feature_importance_dict2 = [dict2[key] for key in top_features_keys]\n",
    "    feature_importance_dict3 = [dict3[key] for key in top_features_keys]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.bar([i - 0.3 for i in range(5)], feature_importance_dict0, width=0.2, color=colours[0], label=models[0])\n",
    "    ax.bar([i - 0.1 for i in range(5)], feature_importance_dict1, width=0.2, color=colours[1], label=models[1])\n",
    "    ax.bar([i + 0.1 for i in range(5)], feature_importance_dict2, width=0.2, color=colours[2], label=models[2])\n",
    "    ax.bar([i + 0.3 for i in range(5)], feature_importance_dict3, width=0.2, color=colours[3], label=models[3])\n",
    "\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(top_features_keys, rotation=90)\n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_ylabel('Importance')\n",
    "    ax.set_title(f\"Top 5 out of {len(sorted_dict0)} averaged features from {models[1]}, {models[2]} and {models[3]}.\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(ax, cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"Plots a confusion matrix.\n",
    "\n",
    "    :param ax: Axis to print confusion matrix on.\n",
    "    :param cm: Pre-made sklearn confusion matrix.\n",
    "    :param classes: Values of predicted classes.\n",
    "    :param normalize: If True, normalizes the values between 0 and 1.\n",
    "    :param title: Title of confusion matrix.\n",
    "    :param cmap: Colour of confusion matrix.\"\"\" \n",
    "    class_labels = [\"CN\", \"AD\"]\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(class_labels, rotation=45) \n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(class_labels)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vote_confusion_matrix(y_predictions, y_real, names, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrices for multiple models along with accuracy scores.\n",
    "\n",
    "    :param y_predictions: A list of lists where each inner list contains predictions for a model.\n",
    "    :param y_real: The real y values.\n",
    "    :param names: A list containing names of the models.\n",
    "    :param title: Title of the plot.\"\"\"\n",
    "    num_models = len(y_predictions)\n",
    "    num_classes = len(set(y_real))\n",
    "    \n",
    "    num_rows = 2\n",
    "    num_cols = 3\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(18, 12))\n",
    "\n",
    "    for i, ax in enumerate(axs.flat[:-1]):\n",
    "        if i < num_models:\n",
    "            cnf_matrix = confusion_matrix(y_real, y_predictions[i])\n",
    "            ax.set_title(names[i])\n",
    "            plot_confusion_matrix(ax, cnf_matrix, classes=range(num_classes), title=names[i])\n",
    "\n",
    "    percentiles = [accuracy_score(y_real, y_predictions[0]),\n",
    "                   accuracy_score(y_real, y_predictions[1]),\n",
    "                   accuracy_score(y_real, y_predictions[2]),\n",
    "                   accuracy_score(y_real, y_predictions[3]),\n",
    "                   accuracy_score(y_real, y_predictions[4])]\n",
    "    labels = [\"Hard Vote\", \"Soft Vote\", \"XGB\", \"LR\", \"RF\"]\n",
    "    plot_percentiles(percentiles, labels, \"Accuracy on validation data in K-fold.\", ax=axs[-1, -1])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc(y_test, y_scores_list):\n",
    "    \"\"\"Plots roc_auc for a model.\n",
    "\n",
    "    :param y_test: A list of y used during k-fold validation.\n",
    "    :param y_scores_list: The predicted y values.\"\"\"\n",
    "    label_binarizer = LabelBinarizer().fit(y_test)\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    models = [\"XGB\", \"LR\", \"RF\"]\n",
    "    for i, y_scores in enumerate(y_scores_list):\n",
    "        y_scores = np.array([array[1] for array in y_scores])\n",
    "        auc_score = roc_auc_score(y_onehot_test, y_scores)\n",
    "\n",
    "        display = RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test.ravel(),\n",
    "            y_scores.ravel(),\n",
    "            name=f\"Model {models[i]}\",\n",
    "            color=f\"C{i}\",\n",
    "            plot_chance_level=True,\n",
    "        )\n",
    "        _ = display.ax_.set(\n",
    "            xlabel=\"False Positive Rate\",\n",
    "            ylabel=\"True Positive Rate\",\n",
    "            title=f\"{models[i]} ROC\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(models, votes, y_true, get_graphs=False):\n",
    "    \"\"\"Plots AUC, ACC and F1 from all models and hard/soft votes.\n",
    "\n",
    "    :param models: Dict with all models.\n",
    "    :param votes: Dict with all votes from the models.\n",
    "    :param y_true: The y values across the K-fold validation.\n",
    "    :param get_graphs: If set to true, prints confusion matrix and roc curve.\"\"\"\n",
    "    hard_votes = [item for sublist in [value for key, value in votes.items() if key.startswith('Hard')] for item in sublist]\n",
    "    soft_votes = [item for sublist in [value for key, value in votes.items() if key.startswith('Soft')] for item in sublist]\n",
    "    xgb_votes = [item for sublist in [value.prediction for key, value in models.items() if key.startswith('XGB')] for item in sublist]\n",
    "    lr_votes = [item for sublist in [value.prediction for key, value in models.items() if key.startswith('LR')] for item in sublist]\n",
    "    rf_votes = [item for sublist in [value.prediction for key, value in models.items() if key.startswith('RF')] for item in sublist]\n",
    "    xgb_prob = [item for sublist in [value.probability for key, value in models.items() if key.startswith('XGB')] for item in sublist]\n",
    "    lr_prob = [item for sublist in [value.probability for key, value in models.items() if key.startswith('LR')] for item in sublist]\n",
    "    rf_prob = [item for sublist in [value.probability for key, value in models.items() if key.startswith('RF')] for item in sublist]\n",
    "    xgb_prob_1 = np.array([array[1] for array in xgb_prob])\n",
    "    lr_prob_1 = np.array([array[1] for array in lr_prob])\n",
    "    rf_prob_1 = np.array([array[1] for array in rf_prob])\n",
    "    soft_prob_pre = np.stack([xgb_prob_1, lr_prob_1, rf_prob_1], axis=0)\n",
    "    soft_prob_1 = np.mean(soft_prob_pre, axis=0)\n",
    "    y_trues = [item for sublist in [list(series) for series in y_true.values()] for item in sublist]\n",
    "\n",
    "    def format_line(label, auc_ovr, acc, f1, mcc):\n",
    "        return f\"{label:<10}\\tAUC OVR: {auc_ovr:<20}\\tACC: {acc:<20}\\tF1: {f1:<20}\\tMCC: {mcc}\"\n",
    "\n",
    "    print(format_line(\"XGB\", roc_auc_score(y_trues, xgb_prob_1), accuracy_score(y_trues, xgb_votes), f1_score(y_trues, xgb_votes), matthews_corrcoef(y_trues, xgb_votes)))\n",
    "    print(format_line(\"LR\", roc_auc_score(y_trues, lr_prob_1), accuracy_score(y_trues, lr_votes), f1_score(y_trues, lr_votes), matthews_corrcoef(y_trues, lr_votes)))\n",
    "    print(format_line(\"RF\", roc_auc_score(y_trues, rf_prob_1), accuracy_score(y_trues, rf_votes), f1_score(y_trues, rf_votes), matthews_corrcoef(y_trues, rf_votes)))\n",
    "    print(format_line(\"Soft Vote\", roc_auc_score(y_trues, soft_prob_1), accuracy_score(y_trues, soft_votes), f1_score(y_trues, soft_votes), matthews_corrcoef(y_trues, soft_votes)))\n",
    "    print(format_line(\"Hard Vote\", roc_auc_score(y_trues, soft_prob_1), accuracy_score(y_trues, hard_votes), f1_score(y_trues, hard_votes), matthews_corrcoef(y_trues, hard_votes)))\n",
    "\n",
    "    if get_graphs:\n",
    "        y_predictions = [hard_votes, soft_votes, xgb_votes, lr_votes, rf_votes]\n",
    "        model_names = [\"Hard Votes\", \"Soft Votes\", \"XGBoost\", \"Logistic Regression\", \"Random Forest\"]\n",
    "        plot_vote_confusion_matrix(y_predictions, y_trues, model_names)\n",
    "        get_roc_auc(y_trues, [xgb_prob, lr_prob, rf_prob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_union_features(state, n_features, df, y):\n",
    "    \"\"\"Quick and dirty solution to get DF union during k-fold validations.\n",
    "\n",
    "    :param state: Dict with all models.\n",
    "    :param n_features: Dict with all votes from the models.\n",
    "    :param df: The y values across the K-fold validation.\n",
    "    :param y: The y values across the K-fold validation.\n",
    "\n",
    "    :return: Dataframe with df_union feature selection.\"\"\"    \n",
    "    lasso = Lasso(alpha=0.001, max_iter=5000, random_state=state)\n",
    "    lr = LogisticRegression(penalty='elasticnet', solver=\"saga\", multi_class=\"multinomial\", max_iter=8000, C=0.01, l1_ratio=0.8, random_state=state)\n",
    "    rf = RandomForestClassifier(n_estimators=350, criterion='gini', max_depth=10, max_features='sqrt', min_samples_leaf=1, random_state=state)\n",
    "    xgb = XGBClassifier(eta=0.2, gamma=0.01, n_estimators=150, reg_lambda=0.85, max_depth=5, min_child_weight=0.45, subsample=0.6, seed=state)\n",
    "\n",
    "    n_features_p0 = 500\n",
    "    step_p0 = 50\n",
    "\n",
    "    #LASSO\n",
    "    rfe_lasso0 = RFE(lasso, n_features_to_select=n_features_p0, step=step_p0)\n",
    "    _ = rfe_lasso0.fit_transform(df, y=y)\n",
    "\n",
    "    #LR\n",
    "    rfe_lr0 = RFE(lr, n_features_to_select=n_features_p0, step=step_p0)\n",
    "    _ = rfe_lr0.fit_transform(df, y=y)\n",
    "\n",
    "    #RF\n",
    "    rfe_rf0 = RFE(rf, n_features_to_select=n_features_p0, step=step_p0)\n",
    "    _ = rfe_rf0.fit_transform(df, y=y)\n",
    "\n",
    "    #XGB\n",
    "    rfe_xgb0 = RFE(xgb, n_features_to_select=n_features_p0, step=step_p0)\n",
    "    _ = rfe_xgb0.fit_transform(df, y=y)\n",
    "\n",
    "    #Feature Names.\n",
    "    lasso_cont0 = rfe_lasso0.get_feature_names_out()\n",
    "    lr_cont0 = rfe_lr0.get_feature_names_out()\n",
    "    rf_cont0 = rfe_rf0.get_feature_names_out()\n",
    "    xgb_cont0 = rfe_xgb0.get_feature_names_out()\n",
    "\n",
    "    #Dataframes based on extracted features.\n",
    "    df_lasso_cont0 = df[lasso_cont0]\n",
    "    df_lr_cont0 = df[lr_cont0]\n",
    "    df_rf_cont0 = df[rf_cont0]\n",
    "    df_xgb_cont0 = df[xgb_cont0]\n",
    "\n",
    "    n_features_p1 = 100\n",
    "    step_p1 = 10\n",
    "\n",
    "    #LASSO\n",
    "    rfe_lasso1 = RFE(lasso, n_features_to_select=n_features_p1, step=step_p1)\n",
    "    _ = rfe_lasso1.fit_transform(df_lasso_cont0, y=y)\n",
    "\n",
    "    #LR\n",
    "    rfe_lr1 = RFE(lr, n_features_to_select=n_features_p1, step=step_p1)\n",
    "    _ = rfe_lr1.fit_transform(df_lr_cont0, y=y)\n",
    "\n",
    "    #RF\n",
    "    rfe_rf1 = RFE(rf, n_features_to_select=n_features_p1, step=step_p1)\n",
    "    _ = rfe_rf1.fit_transform(df_rf_cont0, y=y)\n",
    "\n",
    "    #XGB\n",
    "    rfe_xgb1 = RFE(xgb, n_features_to_select=n_features_p1, step=step_p1)\n",
    "    _ = rfe_xgb1.fit_transform(df_xgb_cont0, y=y)\n",
    "\n",
    "    #Feature Names.\n",
    "    lasso_cont1 = rfe_lasso1.get_feature_names_out()\n",
    "    lr_cont1 = rfe_lr1.get_feature_names_out()\n",
    "    rf_cont1 = rfe_rf1.get_feature_names_out()\n",
    "    xgb_cont1 = rfe_xgb1.get_feature_names_out()\n",
    "\n",
    "    #Dataframes based on extracted features.\n",
    "    df_lasso_cont1 = df[lasso_cont1]\n",
    "    df_lr_cont1 = df[lr_cont1]\n",
    "    df_rf_cont1 = df[rf_cont1]\n",
    "    df_xgb_cont1 = df[xgb_cont1]\n",
    "\n",
    "    #PART 2\n",
    "    n_features_p2 = n_features\n",
    "    step_p2 = 1\n",
    "\n",
    "    #LASSO\n",
    "    rfe_lasso_p2 = RFE(lasso, n_features_to_select=n_features_p2, step=step_p2)\n",
    "    _ = rfe_lasso_p2.fit_transform(df_lasso_cont1, y=y)\n",
    "\n",
    "    #LR\n",
    "    rfe_lr_p2 = RFE(lr, n_features_to_select=n_features_p2, step=step_p2)\n",
    "    _ = rfe_lr_p2.fit_transform(df_lr_cont1, y=y)\n",
    "\n",
    "    #RF\n",
    "    rfe_rf_p2 = RFE(rf, n_features_to_select=n_features_p2, step=step_p2)\n",
    "    _ = rfe_rf_p2.fit_transform(df_rf_cont1, y=y)\n",
    "\n",
    "    #XGB\n",
    "    rfe_xgb_p2 = RFE(xgb, n_features_to_select=n_features_p2, step=step_p2)\n",
    "    _ = rfe_xgb_p2.fit_transform(df_xgb_cont1, y=y)\n",
    "\n",
    "    #Number of features in current dataset.\n",
    "    set_lasso_p2 = set(rfe_lasso_p2.get_feature_names_out())\n",
    "    set_lr_p2 = set(rfe_lr_p2.get_feature_names_out())\n",
    "    set_rf_p2 = set(rfe_rf_p2.get_feature_names_out())\n",
    "    set_xgb_p2 = set(rfe_xgb_p2.get_feature_names_out())\n",
    "\n",
    "    #Dataset after feature extraction.\n",
    "    set_union = set_lasso_p2.union(set_lr_p2, set_rf_p2, set_xgb_p2)\n",
    "    list_union = list(set_union)\n",
    "    df_union = df[list_union]\n",
    "\n",
    "    return df_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(iteration):\n",
    "    \"\"\"Creates 3 new basemodels with names depending on iteration.\n",
    "\n",
    "    :param iteration: Current iteration in K-Fold cross validation.\n",
    "    \n",
    "    :return: List of base models.\"\"\"      \n",
    "    xgboost = XGBClassifier()\n",
    "    xgboost.name = f\"XGB {iteration}\"\n",
    "    lr = LogisticRegression()\n",
    "    lr.name = f\"LR {iteration}\"\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.name = f\"RF {iteration}\"\n",
    "\n",
    "    return [xgboost, lr, rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_and_top_features(X, y):\n",
    "    \"\"\"Finds the features with highest variance in mean value between the y classes.\n",
    "    \n",
    "    :param X: Dataframe with X values.\n",
    "    :param y: Pandas series with target values.\n",
    "    \n",
    "    :return: List with the highets variance features.\"\"\"\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    means = df.groupby(y.name).mean()\n",
    "    means_X = means.iloc[:, :-1]\n",
    "    feature_variances = {}\n",
    "    \n",
    "    for feature in means_X.columns:\n",
    "        sorted_vals = np.sort(means_X[feature])\n",
    "        variance = np.var(sorted_vals)\n",
    "        feature_variances[feature] = variance\n",
    "    \n",
    "    top_features = sorted(feature_variances, key=feature_variances.get, reverse=True)[:5]\n",
    "    \n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(X, y, params, K = 5, n_samples_per_class = 41, bayes = False, corruption = False, union_n = 15):\n",
    "    \"\"\"Main pipeline code that runs K-fold validation.\n",
    "\n",
    "    :param X: Dataframe of X values for training.\n",
    "    :param y: Pandas series of real class values for training \n",
    "    :param params: Parameter settings to get best parameters from.\n",
    "    :param K: Number of K-fold iterations to run.\n",
    "    :param n_samples_per_class: Number of samples per class that should exist after perturbation.\n",
    "    :param bayes: GridSearchCV or BayesSearchCV for best params each k-fold.\n",
    "    :param corruption: Set to smote or perturbation to augment synthesised data.\n",
    "\n",
    "    :return all_models: Dictionary with all models through all k-folds.\n",
    "    :return votes: Dictionary with votes on validation data for all models through all k-folds.\n",
    "    :return y_true: Dictionary with all true y labels through each k-fold.\n",
    "    :return unions: Subset of features used for each k-fold after df_union.\"\"\"  \n",
    "    all_models = {}\n",
    "    votes = {}\n",
    "    y_true = {}\n",
    "    unions = {}\n",
    "    k_fold = StratifiedKFold(n_splits=K, shuffle=True)\n",
    "    n_samples_per_class = y.value_counts().max()\n",
    "    print(\"\\n-------------- Starting K-fold cross validation: --------------\\n\")\n",
    "    for i, (train_index, test_index) in enumerate(k_fold.split(X, y)):\n",
    "        print(f\"\\033[94mK-Fold validation: Iteration {i}.\")\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        print(f'Doing Feature Extraction:\\n')\n",
    "        df_cols = get_union_features(i, union_n, X_train, y_train)\n",
    "        X_train = X_train[df_cols.columns]\n",
    "        X_test = X_test[df_cols.columns]\n",
    "\n",
    "        if corruption == \"smote\":\n",
    "            smote = SMOTE(k_neighbors=5, sampling_strategy={0: n_samples_per_class, 1: n_samples_per_class})\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        if corruption == \"perturbation\":\n",
    "            X_train, y_train = simple_perturbation(X_train, y_train, n_samples_per_class=n_samples_per_class)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        models = create_models(i)\n",
    "        if bayes:\n",
    "            best_params = get_best_params_bayes(X_train, y_train, models, params)\n",
    "        else:\n",
    "            best_params = get_best_params(X_train, y_train, models, params)\n",
    "        y_true[f'{i}'] = y_test\n",
    "        best_models = []\n",
    "        for model in models:\n",
    "            best_model = model.__class__(**best_params[model.name])\n",
    "            best_model.name = model.name\n",
    "            best_model.fit(X_train, y_train)\n",
    "            best_model.prediction = best_model.predict(X_test)\n",
    "            best_model.probability = best_model.predict_proba(X_test)\n",
    "            all_models[best_model.name] = best_model\n",
    "            best_models.append(best_model)\n",
    "        hard = f'Hard vote {i}'\n",
    "        soft = f'Soft vote {i}'\n",
    "        union = f'Union {i}'\n",
    "        votes[hard] = hard_voting(best_models)\n",
    "        votes[soft] = soft_voting(best_models)\n",
    "        unions[union] = df_cols.columns\n",
    "        print(f'\\033[92mHard vote F1 score: Binary - {f1_score(y_test, votes[hard]):<20}  Weighted - {f1_score(y_test, votes[hard], average=\"weighted\"):<20}  MCC - {matthews_corrcoef(y_test, votes[hard])}')\n",
    "        print(f'\\033[92mSoft vote F1 score: Binary - {f1_score(y_test, votes[soft]):<20}  Weighted - {f1_score(y_test, votes[soft], average=\"weighted\"):<20}  MCC - {matthews_corrcoef(y_test, votes[soft])}\\n')\n",
    "    \n",
    "    return all_models, votes, y_true, unions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampMin(SimpleImputer):\n",
    "    \"\"\"\n",
    "        SampMin imputation strategy. Imputes values with the lowest observed value for each column/feature.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.dropna(axis=1, how='all')\n",
    "        self.min_values = X.min() \n",
    "        return X\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.fillna(self.min_values)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        X = self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "def set_max_missingness_per_column(df, threshold=0.5, negative=False):\n",
    "    \"\"\"Removes columns with nan values above a threshold.\n",
    "\n",
    "    :param df: Dataframe-base to remove NaN-heavy columns from.\n",
    "    :param threshold: Threshold percentage of missingness allowed per column.\n",
    "    :param negative: Used if wanting to return all columns with more than threshold missingness.\n",
    "    \n",
    "    :return: New dataframe without NaN-heavy columns.\"\"\" \n",
    "    if not negative:\n",
    "        nan_percentage = (df.isna().mean()).round(4)\n",
    "        selected_columns = nan_percentage[nan_percentage <= threshold].index\n",
    "        return df[selected_columns]\n",
    "    \n",
    "    nan_percentage = (df.isna().mean()).round(4)\n",
    "    selected_columns = nan_percentage[nan_percentage >= threshold].index\n",
    "    return df[selected_columns]\n",
    "\n",
    "def same_shape(dfs : list):\n",
    "    \"\"\"Checks if all dataframes in a list have the same shape.\n",
    "\n",
    "    :param dfs: List of dataframes.\n",
    "    \n",
    "    :return: Boolean value if all dataframes have the same shape.\"\"\"    \n",
    "    return all(df.shape == dfs[0].shape for df in dfs)\n",
    "\n",
    "def get_average_from_m_datasets(dfs : list):\n",
    "    \"\"\"Pools imputed dataset into a single dataframe.\n",
    "\n",
    "    :param dfs: List of dataframes.\n",
    "    \n",
    "    :return: Singular, pooled dataframe.\"\"\"    \n",
    "    if not same_shape:\n",
    "        print(\"Dataframes must have the same shape\")\n",
    "    joined = pd.concat(dfs).reset_index() \n",
    "    return joined.groupby('index').mean()\n",
    "def remove_outliers(df):\n",
    "\n",
    "    \"\"\"Removes outlier from df based on normalized values and inter quartile range.\n",
    "\n",
    "    :param df: Dataframe to clean from outliers.\n",
    "    \n",
    "    :return: Cleaned dataframe.\"\"\" \n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df >= lower_bound) & (df <= upper_bound)]\n",
    "\n",
    "def replace_inf_with_nan(df):\n",
    "    \"\"\"Replace inf, -inf and 100 with nan.\n",
    "\n",
    "    :param df: Dataframe to clean inf, -inf and 100 from.\n",
    "    \n",
    "    :return: Dataframe without inf, -inf and 100.\"\"\"\n",
    "    #df.iloc[:, 11:].replace([0, 0.01], np.nan, inplace=True) \n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    #df.replace(['100', '100.0', '100.00', '100.000'], np.nan, inplace=True)\n",
    "    df = df.applymap(lambda x: np.nan if isinstance(x, float) and x == 100.0 else x)\n",
    "\n",
    "    return df\n",
    "\n",
    "def replace_zero_with_val(df, value=0.01):\n",
    "    \"\"\"Replace zero values with another value.\n",
    "\n",
    "    :param df: Dataframe to remove 0 values from.\n",
    "    :param value: Value to replace 0 with.\n",
    "    \n",
    "    :return: Dataframe without 0 values.\"\"\"\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.replace([0], value, inplace=True)\n",
    "    return df\n",
    "\n",
    "def remapp_id(df):\n",
    "    \"\"\"Remaps the arbitrary Sample_Run_ID with ID.\n",
    "\n",
    "    :param df: Dataframe to remap Sample_Run_ID\n",
    "    \n",
    "    :return: Dataframe with ID.\"\"\" \n",
    "    value_mapping = {value: i + 1 for i, value in enumerate(df['Sample_Run_ID'].unique())}\n",
    "    df['Sample_Run_ID'] = df['Sample_Run_ID'].map(value_mapping)\n",
    "    return df\n",
    "\n",
    "def make_binary(df, y, keepers, tmt):\n",
    "    \"\"\"Removes classes from df and y that are not present in var keepers.\n",
    "\n",
    "    :param df: dataframe (X in classification task)\n",
    "    :param y: class labels\n",
    "    :param keepers: labels that we want to keep\n",
    "    :return: X and y without labels not present in keepers\n",
    "    \"\"\"    \n",
    "    mask = y.isin(keepers)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "    tmt = tmt[mask].reset_index(drop=True)\n",
    "    y = y.replace(keepers[1], 1)\n",
    "    return df, y,tmt\n",
    "\n",
    "def df_cleaner_pipeline(df, csf_type, sample=\"Peptide\", max_missingness=0.5):\n",
    "    \"\"\"Cleaning pipeline used for final datasets. The final pipeline removed \n",
    "    many of the cleaning features initially used, as they were deemed exessive \n",
    "    or even harmfull, introducing biases.\n",
    "\n",
    "    :param df: Dataframe.\n",
    "    :param csf_type: CSF type to get.\n",
    "    :param sample: Setting for protein or peptide, as they have different cleaning methods.\n",
    "    :param max_missingness: Max missingness in each column, removing the other features.\n",
    "    \n",
    "    :return: DF with ComBat applied to it\"\"\"  \n",
    "    df = remapp_id(df)\n",
    "    df = df[df['CSF_type'] == csf_type]\n",
    "    y = df[\"Cortical_biopsy_grouping\"]\n",
    "    tmt_set = df[\"TMT Set\"]\n",
    "    df = df.iloc[:, 11:]\n",
    "\n",
    "    if sample == \"Protein\":\n",
    "        df = replace_zero_with_val(df)\n",
    "\n",
    "    else:\n",
    "        df = replace_inf_with_nan(df)\n",
    "    \n",
    "    df = np.log2(df)\n",
    "    df, y, tmt = make_binary(df, y, [0,2], tmt_set)\n",
    "    df = remove_outliers(df)\n",
    "    df = set_max_missingness_per_column(df, max_missingness)\n",
    "\n",
    "    return df, y, tmt\n",
    "\n",
    "def boxplot_cleaner(df, csf_type, sample=\"Peptide\"):\n",
    "    \"\"\"Cleaning pipeline used for final datasets. The final pipeline removed \n",
    "    many of the cleaning features initially used, as they were deemed exessive \n",
    "    or even harmfull, introducing biases.\n",
    "\n",
    "    :param df: Dataframe.\n",
    "    :param csf_type: CSF type to get.\n",
    "    :param sample: Setting for protein or peptide, as they have different cleaning methods.\n",
    "    \n",
    "    :return: DF with ComBat applied to it\"\"\"  \n",
    "    df = remapp_id(df)\n",
    "    df = df[df['CSF_type'] == csf_type]\n",
    "    y = df[\"Cortical_biopsy_grouping\"]\n",
    "    df = df.iloc[:, 11:]\n",
    "\n",
    "    if sample == \"Protein\":\n",
    "        df = replace_zero_with_val(df)\n",
    "\n",
    "    else:\n",
    "        df = replace_inf_with_nan(df)\n",
    "    \n",
    "    df = np.log2(df)\n",
    "    df = remove_outliers(df)\n",
    "    df = np.exp2(df)\n",
    "\n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Depricated) Proteins, Multiclass, MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataframes, Pooling and classes. (USE FOR PROTEIN)\n",
    "path = \"./Data/data files/imputed\"\n",
    "starts_with = \"protein_lumbar\"\n",
    "dfs = import_csv_files(path=path, starts_with=starts_with)\n",
    "df_main = get_average_from_m_datasets(dfs)\n",
    "\n",
    "df_classes = pd.read_csv(\"./Data/data files/iNPH_data_protein_median.csv\", usecols=[\"Cortical_biopsy_grouping\", \"CSF_type\", \"TMT Set\"])\n",
    "df_classes = df_classes[df_classes[\"CSF_type\"] == \"L\"].reset_index(drop=True)\n",
    "tmt_set = df_classes[\"TMT Set\"]\n",
    "y = df_classes[\"Cortical_biopsy_grouping\"]\n",
    "\n",
    "#df_main = run_combat(df_main, tmt_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingness = 0.0\n",
    "imputer = SampMin(strategy=\"SampMin\", missing_values=np.nan)\n",
    "\n",
    "df_peptide_median_all = pd.read_csv('Data//data files//iNPH_data_peptide_median.csv', index_col=0)\n",
    "df_protein_median_all = pd.read_csv('Data//data files//iNPH_data_protein_median.csv', index_col=0)\n",
    "tmt_set = df_protein_median_all[\"TMT Set\"]\n",
    "\n",
    "df_peptide_lumbar, y_l, tmt_l = df_cleaner_pipeline(df_peptide_median_all, \"L\", max_missingness=missingness)\n",
    "df_peptide_ventricular, y_v, tmt_v = df_cleaner_pipeline(df_peptide_median_all, \"V\", max_missingness=missingness)\n",
    "df_peptide_lumbar = imputer.fit_transform(df_peptide_lumbar)\n",
    "df_peptide_ventricular = imputer.fit_transform(df_peptide_ventricular)\n",
    "\n",
    "df_protein_lumbar, _, aaa = df_cleaner_pipeline(df_protein_median_all, \"L\", sample=\"Protein\", max_missingness=missingness)\n",
    "df_protein_ventricular, _, _ = df_cleaner_pipeline(df_protein_median_all, \"V\", sample=\"Protein\", max_missingness=missingness)\n",
    "df_protein_lumbar = imputer.fit_transform(df_protein_lumbar)\n",
    "df_protein_ventricular = imputer.fit_transform(df_protein_ventricular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peptide_lumbar_combat = run_combat(df_peptide_lumbar, tmt_l)\n",
    "df_peptide_ventricular_combat = run_combat(df_peptide_ventricular, tmt_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Params, Maximized.\n",
    "xgboost_params = {\"eta\": [0.2, 0.3],\n",
    "                  \"min_child_weight\": [0.45],\n",
    "                  \"gamma\": [0.0001, 0.01, 1],\n",
    "                  \"subsample\": [0.4, 0.8],\n",
    "                  \"reg_lambda\": [0.85],\n",
    "                  \"lambda\": [0.5, 0.7],\n",
    "                  \"max_depth\": [4, 6],\n",
    "                  \"n_estimators\": [150],\n",
    "                  \"objective\": [\"binary:logistic\"]}\n",
    "lr_params = {\"penalty\": [\"elasticnet\"],\n",
    "              \"C\": [0.0001, 0.01, 1, 10, 100],\n",
    "              \"solver\": [\"saga\"],\n",
    "              \"max_iter\": [4000, 8000, 12000],\n",
    "              \"tol\": [1e-2, 1e-3, 1e-4],\n",
    "              \"n_jobs\": [-1],\n",
    "              \"l1_ratio\": [0, 0.3, 0.5, 0.8, 1]}\n",
    "rf_params = {\"n_estimators\": [100, 300],\n",
    "             \"criterion\": [\"gini\", \"entropy\"],\n",
    "             \"max_depth\": [5, 10, 20],\n",
    "             \"max_features\": [\"sqrt\"],\n",
    "             \"n_jobs\": [-1],\n",
    "             \"min_samples_leaf\": [1,2,4]}\n",
    "\n",
    "params_grid = [xgboost_params, lr_params, rf_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes Params.\n",
    "xgboost_params = {\"eta\": Real(0.2, 0.3),\n",
    "                  \"min_child_weight\": Real(0.4, 0.5),\n",
    "                  \"gamma\": Real(0.0001, 1),\n",
    "                  \"subsample\": Real(0.4, 0.8),\n",
    "                  \"reg_lambda\": Real(0.8, 0.9),\n",
    "                  \"max_depth\": Integer(1, 10),\n",
    "                  \"n_jobs\": [-1],\n",
    "                  \"n_estimators\": Integer(50, 500),\n",
    "                  \"objective\": [\"binary:logistic\"]}\n",
    "lr_params = {\"penalty\": Categorical([\"elasticnet\"]),\n",
    "              \"C\": Real(0.00001, 100),\n",
    "              \"solver\": Categorical([\"saga\"]),\n",
    "              \"max_iter\": Integer(1000, 12000),\n",
    "              \"tol\": Real(1e-4, 1e-2),\n",
    "              \"n_jobs\": [-1],\n",
    "              \"l1_ratio\": Real(0, 1)}\n",
    "rf_params = {\"n_estimators\": Integer(50, 500),\n",
    "             \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "             \"max_depth\": Integer(5, 50),\n",
    "             \"max_features\": Categorical([\"sqrt\", \"log2\"]),\n",
    "             \"n_jobs\": [-1],\n",
    "             \"min_samples_leaf\": Integer(1,5)}\n",
    "\n",
    "params_bayes = [xgboost_params, lr_params, rf_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes Params, Simplified.\n",
    "xgboost_params = {\"eta\": Real(0.1, 0.5),\n",
    "                  \"max_depth\": Integer(1, 20),\n",
    "                  \"n_jobs\": [-1],\n",
    "                  \"n_estimators\": Integer(5, 500),\n",
    "                  \"objective\": [\"binary:logistic\"]}\n",
    "lr_params = {\"penalty\": Categorical([\"elasticnet\"]),\n",
    "              \"C\": Real(0.000001, 100),\n",
    "              \"solver\": Categorical([\"saga\"]),\n",
    "              \"max_iter\": Integer(1000, 12000),\n",
    "              \"n_jobs\": [-1],\n",
    "              \"l1_ratio\": Real(0, 1)}\n",
    "rf_params = {\"n_estimators\": Integer(50, 500),\n",
    "             \"max_depth\": Integer(2, 20),\n",
    "             \"n_jobs\": [-1],\n",
    "             \"min_samples_leaf\": Integer(1,5)}\n",
    "\n",
    "params_bayes_simple = [xgboost_params, lr_params, rf_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (DEPRICATED) Parameters Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Params, Maximized.\n",
    "xgboost_params = {\"eta\": [0.2, 0.3],\n",
    "                  \"min_child_weight\": [0.45],\n",
    "                  \"gamma\": [0.0001, 0.01, 1],\n",
    "                  \"subsample\": [0.4, 0.8],\n",
    "                  \"reg_lambda\": [0.85],\n",
    "                  \"lambda\": [0.5, 0.7],\n",
    "                  \"max_depth\": [4, 6],\n",
    "                  \"n_estimators\": [150],\n",
    "                  \"objective\": [\"multi:softmax\"],\n",
    "                  \"num_class\": [3]}\n",
    "lr_params = {\"penalty\": [\"elasticnet\"],\n",
    "              \"C\": [0.0001, 0.01, 1, 10, 100],\n",
    "              \"solver\": [\"saga\"],\n",
    "              \"multi_class\": [\"multinomial\"],\n",
    "              \"max_iter\": [4000, 8000, 12000],\n",
    "              \"tol\": [1e-2, 1e-3, 1e-4],\n",
    "              \"n_jobs\": [-1],\n",
    "              \"l1_ratio\": [0, 0.3, 0.5, 0.8, 1]}\n",
    "rf_params = {\"n_estimators\": [100, 300],\n",
    "             \"criterion\": [\"gini\", \"entropy\"],\n",
    "             \"max_depth\": [5, 10, 20],\n",
    "             \"max_features\": [\"sqrt\"],\n",
    "             \"n_jobs\": [-1],\n",
    "             \"min_samples_leaf\": [1,2,4]}\n",
    "\n",
    "params_grid = [xgboost_params, lr_params, rf_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes Params.\n",
    "xgboost_params = {\"eta\": Real(0.2, 0.3),\n",
    "                  \"min_child_weight\": Real(0.4, 0.5),\n",
    "                  \"gamma\": Real(0.0001, 1),\n",
    "                  \"subsample\": Real(0.4, 0.8),\n",
    "                  \"reg_lambda\": Real(0.8, 0.9),\n",
    "                  \"max_depth\": Integer(1, 10),\n",
    "                  \"n_jobs\": [-1],\n",
    "                  \"n_estimators\": Integer(50, 500),\n",
    "                  \"objective\": [\"multi:softmax\"],\n",
    "                  \"num_class\": [3]}\n",
    "lr_params = {\"penalty\": Categorical([\"elasticnet\"]),\n",
    "              \"C\": Real(0.00001, 100),\n",
    "              \"solver\": Categorical([\"saga\"]),\n",
    "              \"multi_class\": Categorical([\"multinomial\"]),\n",
    "              \"max_iter\": Integer(1000, 12000),\n",
    "              \"tol\": Real(1e-4, 1e-2),\n",
    "              \"n_jobs\": [-1],\n",
    "              \"l1_ratio\": Real(0, 1)}\n",
    "rf_params = {\"n_estimators\": Integer(50, 500),\n",
    "             \"criterion\": Categorical([\"gini\", \"entropy\"]),\n",
    "             \"max_depth\": Integer(5, 50),\n",
    "             \"max_features\": Categorical([\"sqrt\", \"log2\"]),\n",
    "             \"n_jobs\": [-1],\n",
    "             \"min_samples_leaf\": Integer(1,5)}\n",
    "\n",
    "params_bayes = [xgboost_params, lr_params, rf_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes Params, Simplified.\n",
    "xgboost_params = {\"eta\": Real(0.1, 0.5),\n",
    "                  \"max_depth\": Integer(1, 20),\n",
    "                  \"n_jobs\": [-1],\n",
    "                  \"n_estimators\": Integer(5, 500),\n",
    "                  \"objective\": [\"multi:softmax\"],\n",
    "                  \"num_class\": [3]}\n",
    "lr_params = {\"penalty\": Categorical([\"elasticnet\"]),\n",
    "              \"C\": Real(0.000001, 100),\n",
    "              \"solver\": Categorical([\"saga\"]),\n",
    "              \"multi_class\": Categorical([\"multinomial\"]),\n",
    "              \"max_iter\": Integer(1000, 12000),\n",
    "              \"n_jobs\": [-1],\n",
    "              \"l1_ratio\": Real(0, 1)}\n",
    "rf_params = {\"n_estimators\": Integer(50, 500),\n",
    "             \"max_depth\": Integer(2, 20),\n",
    "             \"n_jobs\": [-1],\n",
    "             \"min_samples_leaf\": Integer(1,5)}\n",
    "\n",
    "params_bayes_simple = [xgboost_params, lr_params, rf_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold-Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Protein Ventricular\n",
    "df_main = df_protein_ventricular\n",
    "\n",
    "#Run Pipeline\n",
    "all_models, votes, y_true, df_unions = pipeline(df_main, y, params_bayes_simple, bayes=True, corruption=\"smote\", union_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining K-fold results.\n",
    "get_scores(all_models, votes, y_true, get_graphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unions = [lst for lst in df_unions.values()]\n",
    "\n",
    "all_features_in_all_unions = [item for sublist in all_unions for item in sublist]\n",
    "\n",
    "feature_occurences = Counter(all_features_in_all_unions)\n",
    "\n",
    "filtered_counter = Counter({key: count for key, count in feature_occurences.items() if count >= 5})\n",
    "print(list(filtered_counter.keys()))\n",
    "\n",
    "counter_data = feature_occurences\n",
    "\n",
    "frequencies = list(counter_data.values())\n",
    "\n",
    "# Count the occurrences of each frequency\n",
    "frequency_counts = Counter(frequencies)\n",
    "\n",
    "# Prepare data for plotting\n",
    "count_values = list(frequency_counts.keys())\n",
    "counts_occurrences = list(frequency_counts.values())\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(count_values, counts_occurrences)\n",
    "plt.xlabel('Number of times found in each Union', fontsize=15)\n",
    "plt.xticks([1,2,3,4,5])\n",
    "plt.ylabel('Features of features', fontsize=15)\n",
    "plt.title('Frequency of common features in Union Datasets', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proteins\n",
    "missingness = 0.0\n",
    "imputer = SampMin(strategy=\"SampMin\", missing_values=np.nan)\n",
    "\n",
    "df_protein_median_all = pd.read_csv('Data//data files//iNPH_data_protein_median.csv', index_col=0)\n",
    "\n",
    "df_protein_lumbar, _, _ = df_cleaner_pipeline(df_protein_median_all, \"L\", sample=\"Protein\", max_missingness=missingness)\n",
    "df_protein_ventricular, _, _ = df_cleaner_pipeline(df_protein_median_all, \"V\", sample=\"Protein\", max_missingness=missingness)\n",
    "\n",
    "df_p_l, y_test_l = boxplot_cleaner(df_protein_median_all, \"L\", sample=\"Protein\")\n",
    "df_p_v, y_test_v = boxplot_cleaner(df_protein_median_all, \"V\", sample=\"Protein\")\n",
    "\n",
    "#Peptides\n",
    "df_peptide_median_all = pd.read_csv('Data//data files//iNPH_data_peptide_median.csv', index_col=0)\n",
    "\n",
    "df_peptide_lumbar, y_l, _ = df_cleaner_pipeline(df_peptide_median_all, \"L\", sample=\"Peptide\", max_missingness=missingness)\n",
    "df_peptide_ventricular, y_v, _ = df_cleaner_pipeline(df_peptide_median_all, \"V\", sample=\"Peptide\", max_missingness=missingness)\n",
    "\n",
    "df_pep_l, y_test_l = boxplot_cleaner(df_peptide_median_all, \"L\", sample=\"Peptide\")\n",
    "df_pep_v, y_test_v = boxplot_cleaner(df_peptide_median_all, \"V\", sample=\"Peptide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Protein boxplot cleaner (Untempored data)\n",
    "staging(df_p_l, df_p_v, y_test_l, y_test_v, \"P31948\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Protein df_cleaner (tempored) data)\n",
    "staging(df_protein_lumbar, df_protein_ventricular, y_l, y_v, \"P17174\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peptide boxplot cleaner (Untempored data)\n",
    "staging(df_pep_l, df_pep_v, y_test_l, y_test_v, 'P02765..160.165.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peptide df_cleaner (tempored) data)\n",
    "staging(df_peptide_lumbar, df_peptide_ventricular, y_l, y_v, 'O00533..484.502.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
